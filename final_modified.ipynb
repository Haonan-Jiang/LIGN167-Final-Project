{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\28702\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('./Corona_NLP_train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the original text by removing urls\n",
    "def remove_url(original_tweet): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', original_tweet)\n",
    "\n",
    "# clean the original text by removing punctuation\n",
    "def remove_punctuation(text):\n",
    "  # Use a regular expression to remove punctuation\n",
    "  return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# remove stopwords\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "stemmer=PorterStemmer()\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "   text=[word.lower() for word in text.split() if word.lower() not in stop]\n",
    "   return \" \".join(text)\n",
    "\n",
    "# convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "  # Use a translator to convert the text to lowercase\n",
    "  translator = str.maketrans('', '', string.ascii_lowercase)\n",
    "  return text.translate(translator)\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "  # Use a regular expression to remove special characters\n",
    "  return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Define a function to stem a tweet\n",
    "def stem_tweet(tweet):\n",
    "  # Tokenize the tweet into individual words\n",
    "  words = tweet.split()\n",
    "  \n",
    "  # Stem each word and return the result\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "  return \" \".join(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        UserName  ScreenName                      Location     TweetAt  \\\n",
      "0          3799       48751                        London  16-03-2020   \n",
      "1          3800       48752                            UK  16-03-2020   \n",
      "2          3801       48753                     Vagabonds  16-03-2020   \n",
      "3          3802       48754                           NaN  16-03-2020   \n",
      "4          3803       48755                           NaN  16-03-2020   \n",
      "...         ...         ...                           ...         ...   \n",
      "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
      "41153     44952       89904                           NaN  14-04-2020   \n",
      "41154     44953       89905                           NaN  14-04-2020   \n",
      "41155     44954       89906                           NaN  14-04-2020   \n",
      "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
      "\n",
      "                                           OriginalTweet           Sentiment  \n",
      "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
      "1      advice Talk to your neighbours family to excha...            Positive  \n",
      "2      Coronavirus Australia: Woolworths to give elde...            Positive  \n",
      "3      My food stock is not the only one which is emp...            Positive  \n",
      "4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
      "...                                                  ...                 ...  \n",
      "41152  Airline pilots offering to stock supermarket s...             Neutral  \n",
      "41153  Response to complaint not provided citing COVI...  Extremely Negative  \n",
      "41154  You know itÂs getting tough when @KameronWild...            Positive  \n",
      "41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n",
      "41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
      "\n",
      "[41157 rows x 6 columns]>\n",
      "num_extremely_negative =  5481\n",
      "num_negative =  9917\n",
      "num_neutral =  7713\n",
      "num_positive =  11422\n",
      "num_extremely_positive =  6624\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "print(train.head)\n",
    "\n",
    "sentiment = train['Sentiment']\n",
    "\n",
    "mask = np.isin(sentiment, ['Extremely Negative'])\n",
    "num_extremely_negative = np.count_nonzero(mask)\n",
    "print('num_extremely_negative = ', num_extremely_negative)\n",
    "\n",
    "mask = np.isin(sentiment, ['Negative'])\n",
    "num_negative = np.count_nonzero(mask)\n",
    "print('num_negative = ', num_negative)\n",
    "\n",
    "mask = np.isin(sentiment, ['Neutral'])\n",
    "num_neutral = np.count_nonzero(mask)\n",
    "print('num_neutral = ', num_neutral)\n",
    "\n",
    "mask = np.isin(sentiment, ['Positive'])\n",
    "num_positive = np.count_nonzero(mask)\n",
    "print('num_positive = ', num_positive)\n",
    "\n",
    "mask = np.isin(sentiment, ['Extremely Positive'])\n",
    "num_extremely_positive = np.count_nonzero(mask)\n",
    "print('num_extremely_positive = ', num_extremely_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proprocess data\n",
    "# clean data\n",
    "train = train[['OriginalTweet', 'Sentiment']]\n",
    "train.dropna()\n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(remove_url)\n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(remove_punctuation)\n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(remove_stopwords)\n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(to_lowercase)\n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(remove_special_characters) \n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(stem_tweet)\n",
    "tweets = train['OriginalTweet']\n",
    "sentiment = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words model\n",
    "# Create a vocabulary of all the unique words in the tweets\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tweets)\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "# Create a BoW model for each tweet\n",
    "bow_models = vectorizer.transform(tweets)\n",
    "# Stack the vectors for each tweet into a matrix\n",
    "tweet_vectors = bow_models.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentiment to numerical values\n",
    "# Create a label encoder\n",
    "encoder = LabelEncoder()\n",
    "# Fit the encoder to the sentiment dataset\n",
    "encoded_sentiments = encoder.fit_transform(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweet_vectors, encoded_sentiments, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation='relu', input_shape=(tweet_vectors.shape[1], 1)))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1029/1029 [==============================] - 23s 22ms/step - loss: -2142173.2500 - accuracy: 0.1609\n",
      "Epoch 2/10\n",
      "1029/1029 [==============================] - 23s 23ms/step - loss: -28338350.0000 - accuracy: 0.1608\n",
      "Epoch 3/10\n",
      "1029/1029 [==============================] - 23s 22ms/step - loss: -104250736.0000 - accuracy: 0.1608\n",
      "Epoch 4/10\n",
      "1029/1029 [==============================] - 22s 21ms/step - loss: -241620624.0000 - accuracy: 0.1608\n",
      "Epoch 5/10\n",
      "1029/1029 [==============================] - 22s 22ms/step - loss: -447884672.0000 - accuracy: 0.1608\n",
      "Epoch 6/10\n",
      "1029/1029 [==============================] - 23s 22ms/step - loss: -730594944.0000 - accuracy: 0.1608\n",
      "Epoch 7/10\n",
      "1029/1029 [==============================] - 22s 21ms/step - loss: -1097450240.0000 - accuracy: 0.1608\n",
      "Epoch 8/10\n",
      "1029/1029 [==============================] - 23s 22ms/step - loss: -1554220160.0000 - accuracy: 0.1608\n",
      "Epoch 9/10\n",
      "1029/1029 [==============================] - 22s 21ms/step - loss: -2109306496.0000 - accuracy: 0.1608\n",
      "Epoch 10/10\n",
      "1029/1029 [==============================] - 22s 22ms/step - loss: -2771708160.0000 - accuracy: 0.1608\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training set\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# Save the trained model\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: accuracy, loss, prediction performance, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287/1287 [==============================] - 4s 3ms/step\n",
      "Accuracy:  0.8668270282090531\n",
      "Precision:  0.8668270282090531\n",
      "Recall:  1.0\n"
     ]
    }
   ],
   "source": [
    "# use the model to make predictions on the test set\n",
    "test = pd.read_csv('./Corona_NLP_train.csv', encoding='latin-1')\n",
    "test = test[['OriginalTweet', 'Sentiment']]\n",
    "tweets_test = test['OriginalTweet']\n",
    "sentiment_test = test['Sentiment']\n",
    "\n",
    "# Use the vectorizer to create BoW models for the testing set\n",
    "bow_models_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# Use the label encoder to encode the sentiments in the testing set\n",
    "sentiments_test = encoder.transform(sentiment_test)\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "bow_models_test = bow_models_test.toarray()\n",
    "\n",
    "# Use the CNN to make predictions on the testing set\n",
    "predictions = model.predict(bow_models_test)\n",
    "\n",
    "# Convert the encoded sentiments to binary values\n",
    "sentiments_test = (sentiments_test > 0.5).astype(int)\n",
    "predicted_sentiments = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions (0 or 1)\n",
    "predicted_sentiments = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(sentiments_test, predicted_sentiments)\n",
    "\n",
    "# Calculate the precision of the predictions\n",
    "precision = precision_score(sentiments_test, predicted_sentiments)\n",
    "\n",
    "# Calculate the recall of the predictions\n",
    "recall = recall_score(sentiments_test, predicted_sentiments)\n",
    "\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20e5d5acadd8296654b9c7ded1cf664b4d5028691ae0d5cbd254deeb0c75d98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
